import os
import json
import torch
import torch.nn.functional as F
import numpy as np
from datasets import load_dataset
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    matthews_corrcoef,
    confusion_matrix,
)
from collections import Counter
from tqdm import tqdm
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel

# ========================
# ÈÖçÁΩÆÔºöËØ∑Ê†πÊçÆ‰Ω†ÁöÑËÆ≠ÁªÉËÆæÁΩÆ‰øÆÊîπ‰ª•‰∏ãË∑ØÂæÑÂíåÂèÇÊï∞
# ========================

# ÊõøÊç¢‰∏∫‰Ω†ÁöÑÂÆûÈôÖËæìÂá∫ÁõÆÂΩïË∑ØÂæÑÔºàÂåÖÂê´ best_model.pthÔºâ
OUTPUT_DIR = "output_primevul-paired_20251211-181517"  # üëà ‰øÆÊîπËøôÈáåÔºÅ

# Êï∞ÊçÆÈõÜÈÖçÁΩÆÔºàÂ∫î‰∏éËÆ≠ÁªÉÊó∂‰∏ÄËá¥Ôºâ
DATASET_NAME = "codemetic/curve"
SUBSET_NAME = "primevul-paired"
MODEL_NAME = "codemetic/CweBERT-mlm"
MAX_LENGTH = 152
BATCH_SIZE = 64
ROBERTA_LAYERS_TO_CONCAT = (6, 7, 8, 9)

# ËØÑ‰º∞Âì™‰∏™ splitÔºüÂèØÈÄâ: "test", "val", "train"
EVAL_SPLIT = "test"

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ========================
# ÈáçÁî®‰Ω†ÁöÑÊ®°ÂûãÂÆö‰πâÔºàÁÆÄÂåñÁâàÔºå‰ªÖÁî®‰∫éÂä†ËΩΩÔºâ
# ========================


class KappaFaceHead(torch.nn.Module):
    def __init__(self, in_features, num_classes):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.Tensor(num_classes, in_features))
        torch.nn.init.xavier_uniform_(self.weight)
        # Ê≥®ÊÑèÔºöÊ≤°Êúâ biasÔºå‰∏éËÆ≠ÁªÉ‰∏ÄËá¥

    def forward(self, x):
        # ËØÑ‰º∞Êó∂‰∏çÁî®ÔºåÈöè‰æøÂÜô
        return x


class RoBERTaEncoder(torch.nn.Module):
    def __init__(self, model_name, layers_to_concat=(6, 7, 8, 9)):
        super().__init__()
        self.config = RobertaConfig.from_pretrained(
            model_name, output_hidden_states=True
        )
        self.roberta = RobertaModel.from_pretrained(model_name, config=self.config)
        self.layers_to_concat = layers_to_concat
        self.hidden_size = self.config.hidden_size
        self.concat_dim = len(layers_to_concat) * self.hidden_size

    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.hidden_states
        selected_layers = []
        for layer_idx in self.layers_to_concat:
            actual_idx = -(layer_idx)  # e.g., -6 ‚Üí index 7 in 0-based 13-length list
            selected_layers.append(hidden_states[actual_idx])
        concatenated = torch.cat(selected_layers, dim=-1)
        cls_embedding = concatenated[:, 0, :]
        return cls_embedding


class VulnerabilityModel(torch.nn.Module):
    def __init__(self, model_name, num_classes, layers_to_concat=(6, 7, 8, 9)):
        super().__init__()
        self.encoder = RoBERTaEncoder(model_name, layers_to_concat)
        self.feature_dim = self.encoder.concat_dim
        # ÂÖ≥ÈîÆÔºö‰ΩøÁî® KappaFaceHead Ê®°ÂùóÔºåËÄå‰∏çÊòØÁõ¥Êé• Parameter
        self.kappaface_head = KappaFaceHead(self.feature_dim, num_classes)

    def forward(self, input_ids, attention_mask):
        features = self.encoder(input_ids, attention_mask)
        return torch.nn.functional.normalize(features, p=2, dim=-1)


# ========================
# Êï∞ÊçÆÈõÜÁ±ªÔºàÁÆÄÂåñÔºâ
# ========================


class EvalDataset(torch.utils.data.Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        source = str(item["source"])
        label = bool(item["label"])
        cwe = str(item["cwe"])
        class_key = (label, cwe)
        inputs = self.tokenizer(
            source,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
        )
        return {
            "input_ids": inputs["input_ids"].squeeze(0),
            "attention_mask": inputs["attention_mask"].squeeze(0),
            "label": label,
            "cwe": cwe,
            "class_key": class_key,
        }


# ========================
# ËæÖÂä©ÂáΩÊï∞
# ========================


def load_class_mappings(output_dir):
    with open(os.path.join(output_dir, "class_mappings.json"), "r") as f:
        data = json.load(f)
    class_to_idx = {}
    for k, v in data["class_to_idx"].items():
        # k is like "True|CWE-123"
        parts = k.split("|", 1)
        if len(parts) != 2:
            raise ValueError(f"Invalid class key format: {k}")
        label_str, cwe = parts
        label = label_str == "True"  # boolean from string
        class_to_idx[(label, cwe)] = v
    idx_to_class = {v: k for k, v in class_to_idx.items()}
    return class_to_idx, idx_to_class


def compute_thresholds(train_embeddings, train_labels, prototypes, idx_to_class):
    """
    ‰∏∫ÊØè‰∏™Á±ªÂà´ËÆ°ÁÆóÈòàÂÄº œÑÔºåÂü∫‰∫éËÆ≠ÁªÉÈõÜÂÜÖËØ•Á±ªÊ†∑Êú¨ÁöÑÁõ∏‰ººÂ∫¶ÂàÜÂ∏É„ÄÇ
    """
    thresholds = {}
    train_embs_np = train_embeddings.cpu().numpy()
    train_labels_np = train_labels.cpu().numpy()

    for c in range(len(idx_to_class)):
        mask = train_labels_np == c
        if not mask.any():
            thresholds[c] = -1.0  # impossible to accept
            continue

        class_embs = torch.tensor(train_embs_np[mask], device=DEVICE)
        sims = torch.mm(class_embs, prototypes[c].unsqueeze(0).T).squeeze(1)  # (N,)
        sims = sims.cpu().numpy()

        best_f1, best_tau = 0.0, 0.5
        for tau in np.linspace(0.0, 1.0, 100):
            preds = (sims >= tau).astype(int)
            if preds.sum() == 0:
                continue
            # ÁúüÂÆûÊ†áÁ≠æÂÖ®‰∏∫1ÔºàÂõ†‰∏∫ÊòØËØ•Á±ªÊ†∑Êú¨Ôºâ
            f1 = precision_recall_fscore_support(
                np.ones_like(preds), preds, average="binary", zero_division=0
            )[2]
            if f1 > best_f1:
                best_f1 = f1
                best_tau = tau
        thresholds[c] = best_tau
    return thresholds

def custom_collate_fn(batch):
    """
    Custom collate function to prevent stacking of non-tensor fields like class_key, cwe, label.
    """
    elem = batch[0]
    result = {}
    for key in elem:
        if key in ["class_key", "cwe", "label"]:
            # Keep as list of Python objects
            result[key] = [d[key] for d in batch]
        else:
            # Stack tensors (input_ids, attention_mask)
            result[key] = torch.stack([d[key] for d in batch])
    return result


# ========================
# ‰∏ªÂáΩÊï∞
# ========================


def main():
    print(f"Loading model from: {OUTPUT_DIR}")
    checkpoint_path = os.path.join(OUTPUT_DIR, "best_model.pth")
    assert os.path.exists(checkpoint_path), f"Model not found at {checkpoint_path}"

    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=True)
    print("Top keys in checkpoint:")
    for k in list(checkpoint.keys())[:5]:
        print(k)

    # Â∞ùËØïÂä†ËΩΩ class mappingsÔºàÂ¶ÇÊûúËÆ≠ÁªÉËÑöÊú¨Ê≤°‰øùÂ≠òÔºåÈúÄÈáçÂª∫Ôºâ
    try:
        class_to_idx, idx_to_class = load_class_mappings(OUTPUT_DIR)
        num_classes = len(idx_to_class)
        print(f"Loaded {num_classes} classes from class_mappings.json")
    except Exception as e:
        print(f"class_mappings.json not found ({e}). Rebuilding from dataset...")
        dataset = load_dataset(DATASET_NAME, SUBSET_NAME)
        all_data = []
        for split in ["train", "validation", "test"]:
            if split in dataset:
                all_data.extend(dataset[split])
        class_set = set((bool(item["label"]), str(item["cwe"])) for item in all_data)
        class_list = sorted(list(class_set))
        class_to_idx = {cls: idx for idx, cls in enumerate(class_list)}
        idx_to_class = {idx: cls for cls, idx in class_to_idx.items()}
        num_classes = len(class_list)

        # ‰øùÂ≠òÊò†Â∞ÑÔºö‰ΩøÁî® "True|CWE-123" Ê†ºÂºèÁöÑÂ≠óÁ¨¶‰∏≤ key
        class_to_idx_serializable = {
            f"{k[0]}|{k[1]}": v for k, v in class_to_idx.items()
        }
        with open(os.path.join(OUTPUT_DIR, "class_mappings.json"), "w") as f:
            json.dump({"class_to_idx": class_to_idx_serializable}, f)

    num_classes = len(class_to_idx)

    # ===== Âä†ËΩΩÂÆåÊï¥Ê®°Âûã =====
    model = VulnerabilityModel(MODEL_NAME, num_classes, ROBERTA_LAYERS_TO_CONCAT).to(
        DEVICE
    )

    # ===== Âä†ËΩΩÂÆåÊï¥ checkpoint =====
    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=True)
    model.load_state_dict(checkpoint, strict=True)  # ‚úÖ Now keys match!

    model.eval()

    tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)

    # Âä†ËΩΩËÆ≠ÁªÉÈõÜÔºàÁî®‰∫éËÆ°ÁÆó prototypes Âíå thresholdsÔºâ
    print("Loading training set to compute prototypes...")
    train_data = load_dataset(DATASET_NAME, SUBSET_NAME)["val"]
    train_dataset = EvalDataset(train_data, tokenizer, MAX_LENGTH)
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=False,collate_fn=custom_collate_fn
    )

    # ÊèêÂèñËÆ≠ÁªÉÈõÜÂµåÂÖ•
    train_embeddings = []
    train_labels = []
    with torch.no_grad():
        for batch in tqdm(train_loader, desc="Encoding train set"):
            embs = model(
                batch["input_ids"].to(DEVICE), batch["attention_mask"].to(DEVICE)
            )
            train_embeddings.append(embs.cpu())
            label_indices = [class_to_idx[k] for k in batch["class_key"]]
            train_labels.extend(label_indices)
    train_embeddings = torch.cat(train_embeddings, dim=0)
    train_labels = torch.tensor(train_labels)

    # ËÆ°ÁÆó prototypes
    prototypes = torch.zeros(num_classes, train_embeddings.size(1))
    for c in range(num_classes):
        mask = train_labels == c
        if mask.any():
            prototypes[c] = train_embeddings[mask].mean(dim=0)
    prototypes = F.normalize(prototypes, p=2, dim=1).to(DEVICE)

    # ËÆ°ÁÆó thresholds
    print("Computing thresholds per class...")
    thresholds = compute_thresholds(
        train_embeddings, train_labels, prototypes, idx_to_class
    )

    # Âä†ËΩΩËØÑ‰º∞ÈõÜ
    print(f"Loading {EVAL_SPLIT} set for evaluation...")
    if EVAL_SPLIT == "val":
        eval_data = (
            load_dataset(DATASET_NAME, SUBSET_NAME).get("val")
            or load_dataset(DATASET_NAME, SUBSET_NAME)["val"]
        )
    else:
        eval_data = load_dataset(DATASET_NAME, SUBSET_NAME)[EVAL_SPLIT]
    eval_dataset = EvalDataset(eval_data, tokenizer, MAX_LENGTH)
    eval_loader = torch.utils.data.DataLoader(
        eval_dataset, batch_size=BATCH_SIZE, shuffle=False,collate_fn=custom_collate_fn
    )

    # ÂºÄÂßãËØÑ‰º∞
    all_pred_class_indices = []
    all_true_labels = []
    all_true_class_keys = []

    with torch.no_grad():
        for batch in tqdm(eval_loader, desc=f"Evaluating {EVAL_SPLIT}"):
            embs = model(
                batch["input_ids"].to(DEVICE), batch["attention_mask"].to(DEVICE)
            )
            sims = torch.mm(embs, prototypes.T)  # (B, C)
            max_sim, pred_class = sims.max(dim=1)

            for i in range(embs.size(0)):
                tau = thresholds[pred_class[i].item()]
                if max_sim[i] >= tau:
                    all_pred_class_indices.append(pred_class[i].item())
                else:
                    all_pred_class_indices.append(-1)  # unknown

            all_true_labels.extend(batch["label"])
            all_true_class_keys.extend(batch["class_key"])

    # ËΩ¨‰∏∫‰∫åÂàÜÁ±ªÈ¢ÑÊµã
    y_true_binary = np.array(all_true_labels)
    y_pred_binary = []
    for pred_idx in all_pred_class_indices:
        if pred_idx == -1:
            y_pred_binary.append(False)
        else:
            pred_label = idx_to_class[pred_idx][0]
            y_pred_binary.append(pred_label)
    y_pred_binary = np.array(y_pred_binary)

        # ===== Êñ∞Â¢ûÔºöCWE Â§öÂàÜÁ±ªËØÑ‰º∞Ôºà‰ªÖÈíàÂØπÊ≠£Ê†∑Êú¨Ôºâ=====
    y_true_cwe = []
    y_pred_cwe = []

    for pred_idx, true_key in zip(all_pred_class_indices, all_true_class_keys):
        true_label, true_cwe = true_key
        if true_label:  # Âè™ËÄÉËôëÁúüÂÆû‰∏∫ÊºèÊ¥ûÁöÑÊ†∑Êú¨
            if pred_idx == -1:
                # È¢ÑÊµã‰∏∫ unknown ‚Üí ËßÜ‰∏∫È¢ÑÊµãÈîôËØØÔºàÂèØÈÄâÔºö‰πüÂèØË∑≥ËøáÔºå‰ΩÜÈÄöÂ∏∏ËÆ°ÂÖ• FNÔºâ
                y_true_cwe.append(true_cwe)
                y_pred_cwe.append("PREDICTED_AS_UNKNOWN")  # ËôöÊãüÁ±ªÂà´
            else:
                pred_label, pred_cwe = idx_to_class[pred_idx]
                if pred_label:
                    # È¢ÑÊµã‰∏∫ÊºèÊ¥û ‚Üí ËÆ∞ÂΩï CWE
                    y_true_cwe.append(true_cwe)
                    y_pred_cwe.append(pred_cwe)
                else:
                    # ÁúüÂÆûÊòØÊºèÊ¥ûÔºå‰ΩÜÈ¢ÑÊµã‰∏∫ÈùûÊºèÊ¥û ‚Üí CWE ÈîôËØØ
                    y_true_cwe.append(true_cwe)
                    y_pred_cwe.append("PREDICTED_AS_NON_VULN")

    # Ëé∑ÂèñÊâÄÊúâÂîØ‰∏Ä CWE Á±ªÂà´ÔºàÁî®‰∫éÊéíÂ∫èÂíåÊä•ÂëäÔºâ
    unique_cwes = sorted(set(y_true_cwe))

    # ËÆ°ÁÆó per-class metrics
    from sklearn.metrics import classification_report

    # ÁîüÊàêÂàÜÁ±ªÊä•ÂëäÔºàÂåÖÂê´ per-class Âíå macro/microÔºâ
    report = classification_report(
        y_true_cwe, y_pred_cwe, labels=unique_cwes, zero_division=0, output_dict=True
    )

    # ÊèêÂèñ macro Âíå micro
    macro_precision = report["macro avg"]["precision"]
    macro_recall = report["macro avg"]["recall"]
    macro_f1 = report["macro avg"]["f1-score"]
    micro_precision = report["weighted avg"]["precision"]  # Ê≥®ÊÑèÔºömicro = weighted when balanced
    micro_recall = report["weighted avg"]["recall"]
    micro_f1 = report["weighted avg"]["f1-score"]

    # ‰ΩÜ‰∏•Ê†ºÊù•ËØ¥Ôºåmicro Â∫îËØ•Áî® total TP / (TP+FP) Á≠âÔºåsklearn ÁöÑ "micro" ÈúÄË¶ÅÊòæÂºèÊåáÂÆö
    # Êõ¥ÂáÜÁ°ÆÁöÑÂÅöÊ≥ïÔºö
    from sklearn.metrics import precision_recall_fscore_support
    micro_p, micro_r, micro_f1, _ = precision_recall_fscore_support(
        y_true_cwe, y_pred_cwe, average="micro", zero_division=0
    )
    macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(
        y_true_cwe, y_pred_cwe, average="macro", zero_division=0
    )

    # Per-class metrics
    per_class_metrics = {}
    for cwe in unique_cwes:
        if cwe in report:
            per_class_metrics[cwe] = {
                "precision": report[cwe]["precision"],
                "recall": report[cwe]["recall"],
                "f1-score": report[cwe]["f1-score"],
                "support": report[cwe]["support"],
            }

    # ‰øùÂ≠ò CWE ËØÑ‰º∞ÁªìÊûú
    cwe_metrics = {
        "per_class": per_class_metrics,
        "macro": {
            "precision": float(macro_p),
            "recall": float(macro_r),
            "f1": float(macro_f1),
        },
        "micro": {
            "precision": float(micro_p),
            "recall": float(micro_r),
            "f1": float(micro_f1),
        },
    }

    cwe_output_file = os.path.join(OUTPUT_DIR, f"cwe_metrics_{EVAL_SPLIT}.json")
    with open(cwe_output_file, "w") as f:
        json.dump(cwe_metrics, f, indent=4)

    print(f"\nCWE Multi-class Metrics ({EVAL_SPLIT}):")
    print(f"Macro F1: {macro_f1:.4f}, Micro F1: {micro_f1:.4f}")
    print(f"Results saved to: {cwe_output_file}")

    # ËÆ°ÁÆóÊåáÊ†á
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true_binary, y_pred_binary, average="binary", zero_division=0
    )
    acc = accuracy_score(y_true_binary, y_pred_binary)
    tn, fp, fn, tp = confusion_matrix(
        y_true_binary, y_pred_binary, labels=[False, True]
    ).ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    mcc = matthews_corrcoef(y_true_binary, y_pred_binary)

    metrics = {
        "accuracy": float(acc),
        "precision": float(precision),
        "recall": float(recall),
        "f1": float(f1),
        "specificity": float(specificity),
        "mcc": float(mcc),
        "tp": int(tp),
        "tn": int(tn),
        "fp": int(fp),
        "fn": int(fn),
    }

    # ‰øùÂ≠òÁªìÊûú
    eval_output_file = os.path.join(OUTPUT_DIR, f"metrics_{EVAL_SPLIT}_reval.json")
    with open(eval_output_file, "w") as f:
        json.dump(metrics, f, indent=4)

    print(f"\n{EVAL_SPLIT} Metrics:")
    for k, v in metrics.items():
        print(f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}")

    print(f"\nResults saved to: {eval_output_file}")


if __name__ == "__main__":
    main()
